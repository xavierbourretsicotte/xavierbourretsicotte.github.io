<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Data Blog - Xavier Bourret Sicotte</title><link>/</link><description>Data Science, Machine Learning and Statistics, implemented in Python</description><lastBuildDate>Tue, 12 Jun 2018 10:00:00 +0200</lastBuildDate><item><title>Coordinate Descent - Implementation for linear regression</title><link>/coordinate_descent.html</link><description>&lt;p&gt;Description of the algorithm and derivation of the implementation of Coordinate descent for linear regression in Python. Visualization of the "staircase" steps using surface and contour plots as well as a simple animation. This implementation will serve as a step towards more complex use cases such as Lasso.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Tue, 12 Jun 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-12:/coordinate_descent.html</guid></item><item><title>Ridge regression and L2 regularization - Introduction</title><link>/intro_ridge.html</link><description>&lt;p&gt;This notebook is the first of a series exploring regularization for linear regression, and in particular ridge and lasso regression. We will focus here on ridge regression with some notes on the background theory and mathematical derivations and python numpy implementation. Finally we will provide visualizations of the cost functions with and without regularization to help gain an intuition as to why ridge regression is a solution to poor conditioning and numerical stability.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Tue, 12 Jun 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-12:/intro_ridge.html</guid></item><item><title>Choosing the optimal model: Subset selection</title><link>/subset_selection.html</link><description>&lt;p&gt;In this notebook we explore some methods for selecting subsets of predictors. These include best subset and stepwise selection procedures. Code and figures inspired from the book ISLR - chapter 6 - converted into python.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Mon, 11 Jun 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-11:/subset_selection.html</guid></item><item><title>Animations of gradient descent: Ridge regression</title><link>/animation_ridge.html</link><description>&lt;p&gt;Animation of gradient descent in Python using Matplotlib for contour and 3D plots. This particular example uses polynomial regression with ridge regularization&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Fri, 08 Jun 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-08:/animation_ridge.html</guid></item><item><title>Locally Weighted Linear Regression (Loess)</title><link>/loess.html</link><description>&lt;p&gt;Introduction, theory, mathematical derivation of a vectorized implementation of Loess regression. Comparison of different implementations in python and visualization of the result on a noisy sine wave&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Thu, 24 May 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-05-24:/loess.html</guid></item><item><title>Introduction to Optimization and Visualizing algorithms</title><link>/Intro_optimization.html</link><description>&lt;p&gt;Introductory optimization algorithms implemented in Python Numpy and their corresponding visualizations using Matplotlib. A case study comparison between Gradient Descent and Newton's method using the Rosenbrock function.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Sun, 20 May 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-05-20:/Intro_optimization.html</guid><category>Optimization</category><category>Visualization</category></item><item><title>Statistical inference on multiple linear regression</title><link>/stats_inference_2.html</link><description>&lt;p&gt;Statistical inference on multiple linear regression in Python using Numpy, Statsmodel and Sklearn. Implementation of model selection, study of multicolinearity and residuals analysis.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Thu, 17 May 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-05-17:/stats_inference_2.html</guid><category>statistics</category><category>regression</category></item><item><title>Statistical inference on simple linear regression</title><link>/stats_inference_1.html</link><description>&lt;p&gt;Implementation of statistical inference in Python using Numpy, Statsmodel and Sklearn. Detailed breakdown of the formulae used and main assumptions behind the model. Some nice graphs on leverage and influence of observations.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Tue, 15 May 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-05-15:/stats_inference_1.html</guid><category>statistics</category><category>regression</category></item></channel></rss>