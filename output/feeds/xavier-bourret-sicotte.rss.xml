<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Data Blog - Xavier Bourret Sicotte</title><link>/</link><description>Data Science, Machine Learning and Statistics, implemented in Python</description><lastBuildDate>Fri, 22 Jun 2018 13:00:00 +0200</lastBuildDate><item><title>Linear and Quadratic Discriminant Analysis</title><link>/LDA_QDA.html</link><description>&lt;p&gt;Exploring the theory and implementation behind two well known generative classification algorithms. Linear discriminative analysis (LDA) and Quadratic discriminative analysis (QDA). This notebook will use the Iris dataset as a case study for comparing and visualizing the prediction boundaries of the algorithms&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Fri, 22 Jun 2018 13:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-22:/LDA_QDA.html</guid></item><item><title>Gaussian Naive Bayes Classifier: Iris data set</title><link>/Naive_Bayes_Classifier.html</link><description>&lt;p&gt;In this short notebook, we will use the Iris dataset example and implement instead a Gaussian Naive Bayes classifier using Pandas, Numpy and Scipy.stats libraries. Results are then compared to the Sklearn implementation as a sanity check&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Fri, 22 Jun 2018 12:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-22:/Naive_Bayes_Classifier.html</guid></item><item><title>Optimal Bayes Classifier</title><link>/Optimal_Bayes_Classifier.html</link><description>&lt;p&gt;This notebook summarises the theory and the derivation of the optimal bayes classifier. It then provides a comparison of the boundaries of the Optimal and Naive Bayes classifiers.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Fri, 22 Jun 2018 11:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-22:/Optimal_Bayes_Classifier.html</guid></item><item><title>Maximum Likelihood Estimator: Multivariate Gaussian Distribution</title><link>/MLE_Multivariate_Gaussian.html</link><description>&lt;p&gt;The Multivariate Gaussian appears frequently in Machine Learning and this notebook aims to summarize the full derivation of its Maximum Likelihood Estimator&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Fri, 22 Jun 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-22:/MLE_Multivariate_Gaussian.html</guid></item><item><title>Lasso regression: implementation of coordinate descent</title><link>/lasso_implementation.html</link><description>&lt;p&gt;Following the blog post where we have derived the closed form solution for lasso coordinate descent, we will now implement it in python numpy and visualize the path taken by the coefficients as a function of lambda. Our results are also compared to the Sklearn implementation as a sanity check.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Thu, 14 Jun 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-14:/lasso_implementation.html</guid></item><item><title>Ridge and Lasso: visualizing the optimal solutions</title><link>/ridge_lasso_visual.html</link><description>&lt;p&gt;This short notebook offers a visual intuition behind the similarity and differences between Ridge and Lasso regression. In particular we will the contour of the Olrdinary Least Square (OLS) cost function, together with the L2 and L1 cost functions.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Thu, 14 Jun 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-14:/ridge_lasso_visual.html</guid></item><item><title>Lasso regression: derivation of the coordinate descent update rule</title><link>/lasso_derivation.html</link><description>&lt;p&gt;This post describes how to derive the solution to the Lasso regression problem when using coordinate gradient descent. It also provides intuition and a summary of the main properties of subdifferentials and subgradients. Code to generate the figure is in Python.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Wed, 13 Jun 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-13:/lasso_derivation.html</guid></item><item><title>Coordinate Descent - Implementation for linear regression</title><link>/coordinate_descent.html</link><description>&lt;p&gt;Description of the algorithm and derivation of the implementation of Coordinate descent for linear regression in Python. Visualization of the "staircase" steps using surface and contour plots as well as a simple animation. This implementation will serve as a step towards more complex use cases such as Lasso.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Tue, 12 Jun 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-12:/coordinate_descent.html</guid></item><item><title>Ridge regression and L2 regularization - Introduction</title><link>/intro_ridge.html</link><description>&lt;p&gt;This notebook is the first of a series exploring regularization for linear regression, and in particular ridge and lasso regression. We will focus here on ridge regression with some notes on the background theory and mathematical derivations and python numpy implementation. Finally we will provide visualizations of the cost functions with and without regularization to help gain an intuition as to why ridge regression is a solution to poor conditioning and numerical stability.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Tue, 12 Jun 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-12:/intro_ridge.html</guid></item><item><title>Choosing the optimal model: Subset selection</title><link>/subset_selection.html</link><description>&lt;p&gt;In this notebook we explore some methods for selecting subsets of predictors. These include best subset and stepwise selection procedures. Code and figures inspired from the book ISLR - chapter 6 - converted into python.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Mon, 11 Jun 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-11:/subset_selection.html</guid></item><item><title>Animations of gradient descent: Ridge regression</title><link>/animation_ridge.html</link><description>&lt;p&gt;Animation of gradient descent in Python using Matplotlib for contour and 3D plots. This particular example uses polynomial regression with ridge regularization&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Fri, 08 Jun 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-08:/animation_ridge.html</guid></item><item><title>Locally Weighted Linear Regression (Loess)</title><link>/loess.html</link><description>&lt;p&gt;Introduction, theory, mathematical derivation of a vectorized implementation of Loess regression. Comparison of different implementations in python and visualization of the result on a noisy sine wave&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Thu, 24 May 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-05-24:/loess.html</guid></item><item><title>Introduction to Optimization and Visualizing algorithms</title><link>/Intro_optimization.html</link><description>&lt;p&gt;Introductory optimization algorithms implemented in Python Numpy and their corresponding visualizations using Matplotlib. A case study comparison between Gradient Descent and Newton's method using the Rosenbrock function.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Sun, 20 May 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-05-20:/Intro_optimization.html</guid><category>Optimization</category><category>Visualization</category></item><item><title>Statistical inference on multiple linear regression</title><link>/stats_inference_2.html</link><description>&lt;p&gt;Statistical inference on multiple linear regression in Python using Numpy, Statsmodel and Sklearn. Implementation of model selection, study of multicolinearity and residuals analysis.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Thu, 17 May 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-05-17:/stats_inference_2.html</guid><category>statistics</category><category>regression</category></item><item><title>Statistical inference on simple linear regression</title><link>/stats_inference_1.html</link><description>&lt;p&gt;Implementation of statistical inference in Python using Numpy, Statsmodel and Sklearn. Detailed breakdown of the formulae used and main assumptions behind the model. Some nice graphs on leverage and influence of observations.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Tue, 15 May 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-05-15:/stats_inference_1.html</guid><category>statistics</category><category>regression</category></item></channel></rss>