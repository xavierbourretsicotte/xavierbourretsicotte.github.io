<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Data Blog - Xavier Bourret Sicotte</title><link>/</link><description>Data Science, Machine Learning and Statistics, implemented in Python</description><lastBuildDate>Sun, 01 Nov 2020 10:00:00 +0100</lastBuildDate><item><title>AB Testing: effect of early peeking and what to do about it</title><link>/early_peeking.html</link><description>&lt;p&gt;This notebook simulates the impact of early peaking on the results of a conversion rate AB test. Early peaking is loosely defined as the practice of checking and concluding the results of an AB test (i.e. based on its p value, statistical significance, secondary metrics etc) before the target sample size and power are reached.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Sun, 01 Nov 2020 10:00:00 +0100</pubDate><guid isPermaLink="false">tag:None,2020-11-01:/early_peeking.html</guid></item><item><title>Comparing t-test and Mann Whitney test for the means of Gamma</title><link>/t_test_Mann_Whitney.html</link><description>&lt;p&gt;This notebook explores various simulations where we are testing for the difference in means of two independent gamma distributions, by sampling them and computing the means of each sample. We will compare two main test methods: the t-test and the Mann Whitney test.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Thu, 18 Oct 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-10-18:/t_test_Mann_Whitney.html</guid></item><item><title>Gaussian Mixture Model EM Algorithm - Vectorized implementation</title><link>/gaussian_mixture.html</link><description>&lt;p&gt;Implementation of a Gaussian Mixture Model using the Expectation Maximization Algorithm. Vectorized implementation using Python Numpy and comparison to the Sklearn implementation on a toy data set&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Sat, 14 Jul 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-07-14:/gaussian_mixture.html</guid></item><item><title>AdaBoost: Implementation and intuition</title><link>/AdaBoost.html</link><description>&lt;p&gt;This notebook explores the well known &lt;strong&gt;AdaBoost M1&lt;/strong&gt; algorithm which combines several weak classifiers to create a better overall classifier. The notebook consists of three main sections: A review of the Adaboost M1 algorithm and an intuitive visualization of its inner workings. An implementation from scratch in Python, using an Sklearn decision tree stump as the weak classifier. A discussion on the trade-off between the Learning rate and Number of weak classifiers parameters&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Tue, 10 Jul 2018 11:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-07-10:/AdaBoost.html</guid></item><item><title>Tree based models</title><link>/Tree_based_models.html</link><description>&lt;p&gt;This notebook explores chapter 8 of the book "Introduction to Statistical Learning" and aims to reproduce several of the key figures and discussion topics. Of interest is the use of the graphviz library to help visualize the resulting trees and GridSearch from the Sklearn library to plot the validation curves&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Sun, 01 Jul 2018 11:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-07-01:/Tree_based_models.html</guid></item><item><title>Kernels and Feature maps: Theory and intuition</title><link>/Kernel_feature_map.html</link><description>&lt;p&gt;Following the series on SVM, we will now explore the theory and intuition behind Kernels and Feature maps, showing the link between the two as well as advantages and disadvantages. The notebook is divided into two main sections: 1. Theory, derivations and pros and cons of the two concepts. 2. An intuitive and visual interpretation in 3 dimensions&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Thu, 28 Jun 2018 11:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-28:/Kernel_feature_map.html</guid></item><item><title>Support Vector Machine: Python implementation using CVXOPT</title><link>/SVM_implementation.html</link><description>&lt;p&gt;In this second notebook on SVMs we will walk through the implementation of both the hard margin and soft margin SVM algorithm in Python using the well known CVXOPT library. While the algorithm in its mathematical form is rather straightfoward, its implementation in matrix form using the CVXOPT API can be challenging at first. This notebook will show the steps required to derive the appropriate vectorized notation as well as the inputs needed for the API.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Tue, 26 Jun 2018 11:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-26:/SVM_implementation.html</guid></item><item><title>Support Vector Machine: calculate coefficients manually</title><link>/SVM_by_hand.html</link><description>&lt;p&gt;In this first notebook on the topic of Support Vector Machines, we will explore the intuition behind the weights and coefficients by solving a simple SVM problem by hand.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Mon, 25 Jun 2018 11:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-25:/SVM_by_hand.html</guid></item><item><title>Linear and Quadratic Discriminant Analysis</title><link>/LDA_QDA.html</link><description>&lt;p&gt;Exploring the theory and implementation behind two well known generative classification algorithms. Linear discriminative analysis (LDA) and Quadratic discriminative analysis (QDA). This notebook will use the Iris dataset as a case study for comparing and visualizing the prediction boundaries of the algorithms&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Fri, 22 Jun 2018 13:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-22:/LDA_QDA.html</guid></item><item><title>Gaussian Naive Bayes Classifier: Iris data set</title><link>/Naive_Bayes_Classifier.html</link><description>&lt;p&gt;In this short notebook, we will use the Iris dataset example and implement instead a Gaussian Naive Bayes classifier using Pandas, Numpy and Scipy.stats libraries. Results are then compared to the Sklearn implementation as a sanity check&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Fri, 22 Jun 2018 12:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-22:/Naive_Bayes_Classifier.html</guid></item><item><title>Optimal Bayes Classifier</title><link>/Optimal_Bayes_Classifier.html</link><description>&lt;p&gt;This notebook summarises the theory and the derivation of the optimal bayes classifier. It then provides a comparison of the boundaries of the Optimal and Naive Bayes classifiers.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Fri, 22 Jun 2018 11:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-22:/Optimal_Bayes_Classifier.html</guid></item><item><title>Maximum Likelihood Estimator: Multivariate Gaussian Distribution</title><link>/MLE_Multivariate_Gaussian.html</link><description>&lt;p&gt;The Multivariate Gaussian appears frequently in Machine Learning and this notebook aims to summarize the full derivation of its Maximum Likelihood Estimator&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Fri, 22 Jun 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-22:/MLE_Multivariate_Gaussian.html</guid></item><item><title>Lasso regression: implementation of coordinate descent</title><link>/lasso_implementation.html</link><description>&lt;p&gt;Following the blog post where we have derived the closed form solution for lasso coordinate descent, we will now implement it in python numpy and visualize the path taken by the coefficients as a function of lambda. Our results are also compared to the Sklearn implementation as a sanity check.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Thu, 14 Jun 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-14:/lasso_implementation.html</guid></item><item><title>Ridge and Lasso: visualizing the optimal solutions</title><link>/ridge_lasso_visual.html</link><description>&lt;p&gt;This short notebook offers a visual intuition behind the similarity and differences between Ridge and Lasso regression. In particular we will the contour of the Olrdinary Least Square (OLS) cost function, together with the L2 and L1 cost functions.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Thu, 14 Jun 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-14:/ridge_lasso_visual.html</guid></item><item><title>Lasso regression: derivation of the coordinate descent update rule</title><link>/lasso_derivation.html</link><description>&lt;p&gt;This post describes how to derive the solution to the Lasso regression problem when using coordinate gradient descent. It also provides intuition and a summary of the main properties of subdifferentials and subgradients. Code to generate the figure is in Python.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Wed, 13 Jun 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-13:/lasso_derivation.html</guid></item><item><title>Coordinate Descent - Implementation for linear regression</title><link>/coordinate_descent.html</link><description>&lt;p&gt;Description of the algorithm and derivation of the implementation of Coordinate descent for linear regression in Python. Visualization of the "staircase" steps using surface and contour plots as well as a simple animation. This implementation will serve as a step towards more complex use cases such as Lasso.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Tue, 12 Jun 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-12:/coordinate_descent.html</guid></item><item><title>Ridge regression and L2 regularization - Introduction</title><link>/intro_ridge.html</link><description>&lt;p&gt;This notebook is the first of a series exploring regularization for linear regression, and in particular ridge and lasso regression. We will focus here on ridge regression with some notes on the background theory and mathematical derivations and python numpy implementation. Finally we will provide visualizations of the cost functions with and without regularization to help gain an intuition as to why ridge regression is a solution to poor conditioning and numerical stability.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Tue, 12 Jun 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-12:/intro_ridge.html</guid></item><item><title>Choosing the optimal model: Subset selection</title><link>/subset_selection.html</link><description>&lt;p&gt;In this notebook we explore some methods for selecting subsets of predictors. These include best subset and stepwise selection procedures. Code and figures inspired from the book ISLR - chapter 6 - converted into python.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Mon, 11 Jun 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-11:/subset_selection.html</guid></item><item><title>Animations of gradient descent: Ridge regression</title><link>/animation_ridge.html</link><description>&lt;p&gt;Animation of gradient descent in Python using Matplotlib for contour and 3D plots. This particular example uses polynomial regression with ridge regularization&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Fri, 08 Jun 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-06-08:/animation_ridge.html</guid></item><item><title>Locally Weighted Linear Regression (Loess)</title><link>/loess.html</link><description>&lt;p&gt;Introduction, theory, mathematical derivation of a vectorized implementation of Loess regression. Comparison of different implementations in python and visualization of the result on a noisy sine wave&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Thu, 24 May 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-05-24:/loess.html</guid></item><item><title>Introduction to Optimization and Visualizing algorithms</title><link>/Intro_optimization.html</link><description>&lt;p&gt;Introductory optimization algorithms implemented in Python Numpy and their corresponding visualizations using Matplotlib. A case study comparison between Gradient Descent and Newton's method using the Rosenbrock function.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Sun, 20 May 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-05-20:/Intro_optimization.html</guid><category>Optimization</category><category>Visualization</category></item><item><title>Statistical inference on multiple linear regression</title><link>/stats_inference_2.html</link><description>&lt;p&gt;Statistical inference on multiple linear regression in Python using Numpy, Statsmodel and Sklearn. Implementation of model selection, study of multicolinearity and residuals analysis.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Thu, 17 May 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-05-17:/stats_inference_2.html</guid><category>statistics</category><category>regression</category></item><item><title>Statistical inference on simple linear regression</title><link>/stats_inference_1.html</link><description>&lt;p&gt;Implementation of statistical inference in Python using Numpy, Statsmodel and Sklearn. Detailed breakdown of the formulae used and main assumptions behind the model. Some nice graphs on leverage and influence of observations.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Xavier Bourret Sicotte</dc:creator><pubDate>Tue, 15 May 2018 10:00:00 +0200</pubDate><guid isPermaLink="false">tag:None,2018-05-15:/stats_inference_1.html</guid><category>statistics</category><category>regression</category></item></channel></rss>